Data Processing in a Distributed Web Environment

A Description of the problem

	You require a web interface to a web service/application that needs to do one or all of the following:

Allow users to upload data to the service
Allow users access to predefined datasets
Process that data at particular times: 
	on upload
	on demand from user
	scheduled 
	...
Allow users to configure how data is processed
Allow users to download the results
Allow different users access to different levels of quality of service
Allow different standard interfaces to the service

The characteristics of the service should be:

Distibuted - The system should be able to run across distinct nodes on one or more machines
Scalable - Allow extra compute capacity to be added to the system on demand
Performance - Under load the system should remain responsive to the end user
Manageable - The system should be controllable by the system admin to setup different quality of service
Accountable - The system should record usage of compute resources

How this should NOT be done:

Spawning Process in the web server and recording the process id (say in a database), this kills performance, essentially dont use a database to pass messages (use Messaging, see below)

How can this be achieved properly:

Messaging - Passing of messages in a distributed system means the user experience should not be affected by computationally intensive tasks
Queuing - Queues enable the front end to respond to the client, while the back end processed this queue when ready
Scheduling - Processing of data when there is free capacity
Monitoring - The system monitors processes to allow decisions to be taken on how data is processed

What technology is used for implementation:

Web UI - jquery  (http://jquery.com/) and jqueryui (http://jqueryui.com/)
Web Server - Apache2 (http://httpd.apache.org/)
Web framework - Django (https://www.djangoproject.com/)
Task Queuing/ Job Scheduler - Celery (http://celeryproject.org/)
Message Broker - RabbitMQ (http://www.rabbitmq.com/)


How can this be achieved using MServe:

Lets first define what we want to do with data, when i upload any data i want

	1. An md5 checksum of the file calulated
	2. The mimetype of the file generated

in addition when i upload an image i want the following tasks to be executed

	3. A thumbnail of the image generated

also i want to 

	4. periodically pull twitter updates about a topic


So lets define those tasks

from celery.decorators import task
from celery.task import periodic_task

@task
def mimefile(file):
        mimedetector = magic.open(magic.MAGIC_MIME)
	mimetype = mimedetector.file(file)
	return {"message":"Mimetype successful","mimetype":mimetype}

@task
def md5(file):
	md5 = hashlib.md5(file)
	return {"message":"MD5 successful","md5":md5}

@task
def thumbnail(image,output):
	height = 210
	width  = 120
	im.thumbnail((width,height))
	im.save(output)
	return {"Thumbnail successful"}


@periodic_task(run_every=crontab(hours=7, minute=30))
def twitter_topic(topic):
	result = twiiterapi.(....)
	return {"message":"Twitter successful","result":result}

How to submit tasks: 

At some point after the user has uploaded a file, we use task.delay() to submit the task asynchronously via celery
We pass this id back the client so the status of the task can be queried, thus giving a responsive feel to the UI

def process_file(uploaded_file)
	task = []
	task1 = mimefile.delay(uploaded_file)
	task2 = md5.delay(uploaded_file)
	tasks.push(task1)
	tasks.push(task2)
	if is_image(uploaded_file)
		task3 = thumbnail(uploaded_file,someoutputfile)
		tasks.push(task3)

	# These asynchronous tasks return an id : <AsyncResult: 889143a6-39a2-4e52-837b-d80d33efb22d>
	return tasks

Then at some point in the future when the user ask on the status we check the state of the task

def get_task_state(id)
	return AsyncResult(id)

How do these tasks the processed?

In a simple setup we have declare 2 queues, a normal queue and a high proirity queue

# Details of our rabbitMQ broker
BROKER_HOST = "localhost"
BROKER_PORT = 5672
BROKER_USER = "myuser"
BROKER_PASSWORD = "mypassword"
BROKER_VHOST = "myvhost"

# Details of our Queue
CELERY_DEFAULT_QUEUE = "normal_tasks"
CELERY_QUEUES = {
    "normal_tasks": {
        "binding_key": "normal.#",
    },
    "priority_tasks": {
        "binding_key": "priority.#",
    },
}

By default the queues go to the queue named 'normal_tasks' but at runtime we can override this

def process_priority_file(uploaded_file)
	task = thumbnail.delay(uploaded_file,{"routing_key":"priority.task"})

This forces the task onto the 'priority_tasks' queue

So how do we start the celery processing queues

node1:> manage.py celeryd_multi start -n:1 normal -n:2 priority -Q:1 normal_tasks -Q:2 priority_tasks -c 5

This starts two worker nodes (normal and priority) bound to (normal_tasks, priority_tasks) respectively, each node has 5 conncurrent processes taking messages from the queues,
this can be configured appropriate to the number of cores on the machine

If in the future we need to scale out to more nodes we can start more worker nodes on another machine
node2:> manage.py celeryd_multi start -n normal -Q:1 normal_tasks -c 4

In a more complicated setup we may have a dedicated peice of hardware, for which we can declare its own queue

CELERY_QUEUES += {
    "hardware_tasks": {
        "binding_key": "hardware.#",
    },
}

The on the machine with the hardware we declare a single conccurrency worker node

node2:> manage.py celeryd_multi start -n hardware -Q:1 hardware_tasks -c 1

So how can we scale even further

Next step is to allow the deployment of new nodes into the 'cloud', so a new worker node could be deployed into a virtual environment and connected to the broker to start processing jobs from the queue, this would allow scalability.


How the user accesses their data

HTML/Web interface: this is the simplest way of getting data into the service, ajax file uploads and http downloads allow an interactive UI
Webdav: A standard way of accessing data on the web




http://celeryproject.org/
http://django-celery.readthedocs.org/en/latest/getting-started/first-steps-with-django.html
http://blogs.digitar.com/jjww/2009/01/rabbits-and-warrens/
http://ask.github.com/celery/getting-started/introduction.html
